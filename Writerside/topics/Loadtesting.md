# Loadtesting

## Automatic Scaling

Kubernetes can automatically scale the number of pods based on the CPU usage of the pods. To enable this, you can set up a Horizontal Pod Autoscaler (HPA) in Kubernetes. An HPA will automatically scale the number of pods based on the CPU usage of the pods.

### Creating the Autoscaler

I created an autoscaler for the api-gateway deployment. The autoscaler was configured to maintain a CPU utilization of 50%. It could scale the number of pods between 1 and 10 based on the CPU usage.

```Bash
kubectl autoscale deployment api-gateway --cpu-percent=50 --min=1 --max=10
```

### Checking the Autoscaler Status

I checked the current status of the HorizontalPodAutoscaler using the following command:

```Bash
kubectl get hpa
```

It gave me the following output:

```Bash
NAME           REFERENCE                 TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
api-gateway    Deployment/api-gateway    0%/50%   1         10        1          160m
post-service   Deployment/post-service   0%/50%   1         10        1          166m
```

### Creating a Load Generator

To test the autoscaling, I created a load on the api-gateway using a load generator. The load generator was a simple busybox pod running a loop that continuously sent requests to the api-gateway.

```Bash
kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://api-gateway:8080/api/posts; done"
```

### Monitoring the Autoscaler

I monitored the status of the HorizontalPodAutoscaler to see how it reacted to the increased load. I used the --watch flag to get real-time updates.

```Bash
kubectl get hpa api-gateway --watch
```

The output showed the current CPU utilization and the number of pods being scaled up or down in response to the load.

```Bash
NAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
api-gateway    Deployment/api-gateway    1%/50%    1         10        1          16m
post-service   Deployment/post-service   1%/50%    1         10        1          5m54s
api-gateway    Deployment/api-gateway    0%/50%    1         10        1          16m
post-service   Deployment/post-service   16%/50%   1         10        1          6m46s
api-gateway    Deployment/api-gateway    23%/50%   1         10        1          17m
post-service   Deployment/post-service   14%/50%   1         10        1          7m46s
api-gateway    Deployment/api-gateway    19%/50%   1         10        1          18m
post-service   Deployment/post-service   13%/50%   1         10        1          8m46s
api-gateway    Deployment/api-gateway    19%/50%   1         10        1          19m
post-service   Deployment/post-service   14%/50%   1         10        1          9m46s
api-gateway    Deployment/api-gateway    18%/50%   1         10        1          20m
post-service   Deployment/post-service   13%/50%   1         10        1          10m
api-gateway    Deployment/api-gateway    17%/50%   1         10        1          21m
post-service   Deployment/post-service   13%/50%   1         10        1          11m
api-gateway    Deployment/api-gateway    19%/50%   1         10        1          22m
post-service   Deployment/post-service   14%/50%   1         10        1          12m
api-gateway    Deployment/api-gateway    18%/50%   1         10        1          23m
post-service   Deployment/post-service   13%/50%   1         10        1          13m
post-service   Deployment/post-service   14%/50%   1         10        1          14m
api-gateway    Deployment/api-gateway    19%/50%   1         10        1          25m
post-service   Deployment/post-service   18%/50%   1         10        1          15m
api-gateway    Deployment/api-gateway    25%/50%   1         10        1          26m
```

The output shows that the CPU utilization of the api-gateway deployment increased to 25%, which wasn't enough to trigger the autoscaler to scale up the number of pods.

### Checking the Deployment

Finally, I checked the api-gateway deployment to see the number of pods being scaled up or down in response to the load.

```Bash
kubectl get deployment api-gateway
```

This process allowed me to effectively test the scaling of the application within a Minikube environment.

## Postman Local Load Testing

I used Postman desktop application to perform some load testing on the api-gateway which ran in my local Minikube environment. I ran the spike test for 3 minutes with 50 virtual users. These were the results:

Total requests sent
: 3,619

Requests/second
: 19.27

Avg. response time
: 50 ms

These results themselves seem to be fine, but after analyzing the performance graph that's generated by Postman, I noticed an increase in response time during the spike.

![postman_local_loadtest_2.png](postman_local_loadtest_2.png)

The grey line representing the virtual users amount and the blue line representing the response time, we can see that the response time increased as the number of virtual users increased. 

Although it's not a huge spike, since the average response time went from an average of around `13 ms` to an average of `170 ms` during the spike. This might be an early indicator that the application might not be able to handle the load effectively.

The output of the HPA shows that the CPU utilization of the api-gateway deployment increased to 25%, which wasn't enough to trigger the autoscaler to scale up the number of pods. This might be the reason for the increase in response time during the spike.

```Bash
NAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
api-gateway    Deployment/api-gateway    0%/50%    1         10        1          139m
post-service   Deployment/post-service   0%/50%    1         10        1          128m
api-gateway    Deployment/api-gateway    1%/50%    1         10        1          140m
post-service   Deployment/post-service   2%/50%    1         10        1          129m
api-gateway    Deployment/api-gateway    14%/50%   1         10        1          141m
post-service   Deployment/post-service   25%/50%   1         10        1          130m
api-gateway    Deployment/api-gateway    3%/50%    1         10        1          142m
post-service   Deployment/post-service   6%/50%    1         10        1          131m
api-gateway    Deployment/api-gateway    1%/50%    1         10        1          143m
post-service   Deployment/post-service   1%/50%    1         10        1          132m
```

## Artillery.io Load Testing (CI/CD)

Currently in my CI/CD pipeline, I have a job that runs a load test using Artillery.io. The load test hits the endpoints in the API Gateway, which is the entry point to the microservices. All I needed to do, besides create the workflow for the load test, is create a config file for Artillery, describing how many users it needs to generate, which endpoints to hit, for how long and more.

This is what a simple Artillery config file looks like:

```yaml
config:
  target: 'http://192.168.144.150:8080/api'
  phases:
    - duration: 30
      arrivalRate: 100
  ensure:
    maxErrorRate: 1
    max: 500
scenarios:
  - name: 'Get a list of posts'
    flow:
      - get:
          url: '/posts'
          expect:
            - statusCode: 200
```

The logs in Github Actions will output a URL to a dashboard in Artillery.io where you can see the results of the load test. This is a great way to monitor the performance of the application over time:

```Bash
Test run id: trzxe_njcjrn8n6cky3rfp8e5wzngmwd9dj_ntnh
Artillery Cloud reporting is configured for this test run
Run URL: https://app.artillery.io/load-tests/trzxe_njcjrn8n6cky3rfp8e5wzngmwd9dj_ntnh
Phase started: unnamed (index: 0, duration: 30s) 17:24:48(+0200)
```

The dashboard will display various metrics such as the number of requests, the response time, the number of errors and more. it will also generate interactive graphs which can be used to analyze the performance of the application, this is such a graph:

![artillery_loadtest_1.png](Artillery_Loadtest_1.png)

## Spawning replicas under load

### Installing Metrics Server

In order to scale my application under load, a couple things need to be setup. First, a metrics-server needs to be installed in the cluster. In minikube this can be done using the following command:

```Bash
minikube addons enable metrics-server
```

In a production environment, the metrics-server can be installed in various ways, although I used Manifests to install it in my k3s cluster. The metrics-server will collect metrics from the pods in the cluster and make them available to the Horizontal Pod Autoscaler (HPA).

Installing a metrics-server in k3s can be done using the following command:

```Bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

### Resource limits

After the metrics-server is installed, a small change needs to be made to the existing deployment files. A `resources` section needs to be added and include the following:

```yaml
spec:
  containers:
    - image: tendeza/sem6-post-service:latest
      name: post-service
      ports:
        - containerPort: 8081
      resources:
        limits:
            cpu: "1000m"
            memory: "1000Mi"
```

This gives a resource limit to the pod, which is needed for the HPA to keep track of which percentage of the CPU is being used.

### Creating the HPA

After that change is made, the HPA can be created. The HPA will automatically scale the number of pods based on the CPU usage of the pods. I created an HPA for the api-gateway and post-service deployments. The HPAs were configured to maintain a CPU utilization of 25%. It could scale the number of pods between 1 and 10 based on the CPU usage.

This is how the HPA for the api-gateway deployment looks like:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-gateway-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-gateway
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 25
```

After creating the file it can be applied to the cluster like every other Kubernetes resource using `kubectl apply -f hpa.yaml`.

### Monitoring the HPA

To check whether everything was created correctly and the HPA is working, the status of the HPA can be checked using the following command:

```Bash
kubectl get hpa
```

This should output your HPA and under `TARGETS` it should show the current CPU utilization and the target CPU utilization, instead of `unknown`.

### Load testing

When running a load test on the application, the HPA should automatically scale the number of pods based on the CPU usage of the pods. This can be monitored using the `kubectl get hpa --watch` command.

```Bash
NAME               REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
api-gateway-hpa    Deployment/api-gateway    1%/25%    1         10        1          7m19s
api-gateway-hpa    Deployment/api-gateway    10%/25%   1         10        1          8m19s
post-service-hpa   Deployment/post-service   10%/5%    1         10        1          8m19s
post-service-hpa   Deployment/post-service   10%/5%    1         10        2          8m34s
api-gateway-hpa    Deployment/api-gateway    4%/25%    1         10        1          9m19s
post-service-hpa   Deployment/post-service   5%/5%     1         10        2          9m19s
```

To test the spawning of the replicas, I lowered the maximum CPU usage to `5%`, so that the application would scale quicker. 

The above output shows that the CPU utilization of the post-service deployment increased to `10%`, which was enough to trigger the autoscaler to scale up the number of pods to 2. This shows that the HPA is working correctly and the application is scaling as expected.